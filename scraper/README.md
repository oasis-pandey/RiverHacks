# NASA Publication SmartScraper - Organized Edition

This project reads NASA publication data from GitHub CSV, scrapes each website using enhanced AI-powered content extraction, and stores results in structured JSON files and databases.

## ğŸ“ Project Structure

```
Alchemists/
â”œâ”€â”€ scraperScripts/          # All scraper and database scripts
â”‚   â”œâ”€â”€ run_scrape.py       # Main scraping script
â”‚   â”œâ”€â”€ store_to_database.py # Database storage script
â”‚   â””â”€â”€ setup_database.py   # Database setup script
â”œâ”€â”€ Data/                   # All output data files
â”‚   â”œâ”€â”€ batch_01.json      # Processed batch files
â”‚   â”œâ”€â”€ batch_02.json      # ...
â”‚   â””â”€â”€ ...                # Additional scraped data
â”œâ”€â”€ src/smartscraper/       # Core scraper library
â”‚   â””â”€â”€ scraper.py         # Scraping logic and AI refinement
â”œâ”€â”€ tests/                  # Unit tests
â”œâ”€â”€ .venv/                  # Python virtual environment
â”œâ”€â”€ .env                    # Database configuration
â”œâ”€â”€ run_scraper.py         # Master control script
â””â”€â”€ requirements.txt       # Dependencies
```

## ğŸš€ Quick Start

### Easy Way (Recommended):
```bash
python run_scraper.py
```
This opens an interactive menu with all options.

### Manual Commands:

**Small test (10 entries):**
```bash
./.venv/bin/python scraperScripts/run_scrape.py --csv-url https://raw.githubusercontent.com/jgalazka/SB_publications/main/SB_publication_PMC.csv --count 10 --output Data/test_10.json --delay 0.5
```

**Full scrape (all ~600 entries):**
```bash
./.venv/bin/python scraperScripts/run_scrape.py --csv-url https://raw.githubusercontent.com/jgalazka/SB_publications/main/SB_publication_PMC.csv --output Data/complete_scrape.json --delay 1.0 --batch-size 25
```

## ğŸ—„ï¸ Database Storage

1. **Setup database:**
```bash
./.venv/bin/python scraperScripts/setup_database.py
```

2. **Store data:**
```bash
./.venv/bin/python scraperScripts/store_to_database.py
```

## âœ¨ Features

- **Organized Structure**: Clean separation of scripts, data, and source code
- **AI-Enhanced Content**: Extracts key findings, methodology, objectives
- **PMC Integration**: Special handling for PubMed Central articles
- **Batch Processing**: Saves progress incrementally
- **Database Ready**: Structured storage in Neon PostgreSQL
- **Error Resilience**: Continues processing despite individual failures
- **Rate Limiting**: Respectful to source servers

## ğŸ“Š Output

All scraped data is saved in the `Data/` directory:
- Individual batch files for progress tracking
- Combined output files for complete datasets
- Rich metadata including authors, DOIs, abstracts, full text
- AI-extracted insights and content quality metrics
